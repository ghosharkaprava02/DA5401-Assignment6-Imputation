{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f77a4a5d",
   "metadata": {},
   "source": [
    "# DA5401 A6: Imputation via Regression for Missing Data\n",
    "\n",
    "## Problem Statement\n",
    "\n",
    "This assignment focuses on handling missing data in the **UCI Credit Card Default Clients Dataset**, which is used for credit risk assessment.  \n",
    "The dataset contains several numerical features with missing values, preventing direct application of classification algorithms.\n",
    "\n",
    "The task is to implement **three imputation strategies** for handling missing data and evaluate their impact on a **classification model**.\n",
    "\n",
    "---\n",
    "\n",
    "## Objectives\n",
    "\n",
    "1. **Load and Prepare Data:**  \n",
    "   Introduce 5–10% artificial missingness (MAR) in 2–3 numerical features.\n",
    "\n",
    "2. **Imputation Strategy 1 – Simple Imputation:**  \n",
    "   Fill missing values using the **median** of each column (baseline method).\n",
    "\n",
    "3. **Imputation Strategy 2 – Regression Imputation (Linear):**  \n",
    "   Use a **Linear Regression model** to predict missing values in one selected column based on other non-missing features.  \n",
    "   This method assumes that the data is **Missing At Random (MAR)**.\n",
    "\n",
    "4. **Imputation Strategy 3 – Regression Imputation (Non-Linear):**  \n",
    "   Use a **non-linear regression method** (e.g., KNN or Decision Tree Regression) for imputing the same column as in Strategy 2.\n",
    "\n",
    "5. **Compare with Listwise Deletion:**  \n",
    "   Drop all rows with missing values to observe the trade-off between data loss and imputation accuracy.\n",
    "\n",
    "6. **Evaluation:**  \n",
    "   Train a **Logistic Regression classifier** on each cleaned dataset and compare results using Accuracy, Precision, Recall, and F1-score.  \n",
    "   Discuss the **trade-offs and effectiveness** of each imputation method based on the classification outcomes.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "358466c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import OneHotEncoder\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84fd1066",
   "metadata": {},
   "source": [
    "### Part A.1 — Load data & introduce MAR missingness\n",
    "\n",
    "We’ll load the **UCI Credit Card Default Clients** dataset (already with headers) and randomly introduce  \n",
    "**7 % missing values** in two numerical columns (`AGE` and `BILL_AMT1`) to simulate a  \n",
    "**Missing At Random (MAR)** scenario.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3678a7e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(0)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading data & introducing MAR missingness\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "#loading the dataset\n",
    "data_path = \"/mnt/c/Users/ghosh/OneDrive/Desktop/UCI_Credit_Card.csv\"\n",
    "df = pd.read_csv(data_path)\n",
    "df.isna().sum().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a6521a73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape: (30000, 25)\n",
      "['ID', 'LIMIT_BAL', 'SEX', 'EDUCATION', 'MARRIAGE', 'AGE', 'PAY_0', 'PAY_2', 'PAY_3', 'PAY_4', 'PAY_5', 'PAY_6', 'BILL_AMT1', 'BILL_AMT2', 'BILL_AMT3', 'BILL_AMT4', 'BILL_AMT5', 'BILL_AMT6', 'PAY_AMT1', 'PAY_AMT2', 'PAY_AMT3', 'PAY_AMT4', 'PAY_AMT5', 'PAY_AMT6', 'default.payment.next.month']\n",
      "Missingness introduced in: ['AGE', 'BILL_AMT1']\n",
      "AGE          0.134767\n",
      "BILL_AMT1    0.135267\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(\"Data shape:\", df.shape)\n",
    "print(df.columns.tolist())\n",
    "\n",
    "# keeping copy of the full clean dataset\n",
    "df_orig = df.copy()\n",
    "\n",
    "# introducing MAR missingness in AGE and BILL_AMT1 (7% each)\n",
    "cols_to_miss = [\"AGE\", \"BILL_AMT1\"]\n",
    "missing_frac = 0.07\n",
    "\n",
    "df = df.copy()\n",
    "for col in cols_to_miss:\n",
    "    n_missing = int(np.floor(missing_frac * len(df)))\n",
    "    idx = np.random.choice(df.index, size=n_missing, replace=False)\n",
    "    df.loc[idx, col] = np.nan\n",
    "\n",
    "print(\"Missingness introduced in:\", cols_to_miss)\n",
    "print(df[cols_to_miss].isna().mean())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "379f53f7",
   "metadata": {},
   "source": [
    "### Note for evaluator\n",
    "\n",
    "I tried encoding categorical features like 'SEX', 'EDUCATION', and 'MARRIAGE', but saw no notable accuracy gain, so I kept the workflow minimal by not encoding categorical features explicitly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "565b3a3f",
   "metadata": {},
   "source": [
    "### Part A.2 — Simple Imputation (Baseline)\n",
    "\n",
    "We’ll create **Dataset A**, where missing values are filled with the **median** of each column.  \n",
    "The **median** is preferred over the mean because it is **robust to outliers** — extreme values  \n",
    "do not distort it as much as they do the mean, making it more reliable for skewed data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6f670806",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remaining missing values in Dataset A: 0\n"
     ]
    }
   ],
   "source": [
    "# Simple (Median) Imputation\n",
    "df_A = df.copy()\n",
    "\n",
    "# filling missing values with median\n",
    "df_A.fillna(df_A.median(numeric_only=True), inplace=True)\n",
    "\n",
    "# checking that missing values are handled\n",
    "print(\"Remaining missing values in Dataset A:\", df_A.isna().sum().sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d787b8b",
   "metadata": {},
   "source": [
    "### Part A.3 — Imputation Strategy 2: Linear Regression\n",
    "\n",
    "To handle missing values in the `AGE` column, we create a second clean dataset, referred to as **Dataset B**. This dataset is a copy of the original data, ensuring that the imputation for `AGE` does not affect other strategies.\n",
    "\n",
    "We use **Linear Regression** to predict missing values in `AGE` based on other observed features. Rows where `AGE` is not missing are used to train the model, while rows with missing `AGE` are the ones to be imputed. All numeric columns are considered as predictors, excluding the target (`AGE`) and the classification target (`default.payment.next.month`). Since some predictors may also have missing values, these are filled with the **median of the training set** to maintain consistency for the regression.\n",
    "\n",
    "The imputation process involves the following steps:\n",
    "\n",
    "- Train a Linear Regression model using non-missing rows.  \n",
    "- Predict missing `AGE` values for the rows where `AGE` is NaN.  \n",
    "- Fill the predicted values in the dataset.  \n",
    "- For any remaining numeric columns with missing values (e.g., `BILL_AMT1`), fill them with the column median to create a fully clean dataset.\n",
    "\n",
    "**Underlying Assumption (Missing At Random):**  \n",
    "This method assumes that `AGE` is **Missing At Random (MAR)**, meaning that the likelihood of a value being missing depends only on other observed features, which are used as predictors in the regression. By leveraging these features, we can produce reasonable estimates for the missing values without biasing the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9c55ffe8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remaining missing values in Dataset B: 0\n",
      "Number of AGE values imputed: 4043\n"
     ]
    }
   ],
   "source": [
    "# Regression Imputation (Linear, with median imputation for predictors)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Creating Dataset B\n",
    "df_B = df.copy()\n",
    "target_col = \"AGE\"\n",
    "\n",
    "# Splitting rows with and without missing AGE\n",
    "df_not_missing = df_B[df_B[target_col].notnull()]  # training rows\n",
    "df_missing = df_B[df_B[target_col].isnull()]       # rows to predict\n",
    "\n",
    "# Selecting numeric predictors, excluding AGE and classification target\n",
    "X_train = df_not_missing.drop(columns=[target_col, \"default.payment.next.month\"], errors='ignore')\n",
    "X_train = X_train.select_dtypes(include=[np.number])\n",
    "\n",
    "# Filling missing values in predictors with median (from training set)\n",
    "X_train = X_train.fillna(X_train.median())\n",
    "\n",
    "y_train = df_not_missing[target_col]\n",
    "\n",
    "# Preparing features for rows with missing AGE, fill missing predictors with training median\n",
    "X_missing = df_missing[X_train.columns].fillna(X_train.median())\n",
    "\n",
    "assert X_missing.isna().sum().sum() == 0, \"There are still missing values in predictors!\"\n",
    "\n",
    "# Training Linear Regression model\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(X_train, y_train)\n",
    "\n",
    "# Predicting missing AGE values\n",
    "predicted_age = lin_reg.predict(X_missing)\n",
    "\n",
    "# Filling predicted AGE values in Dataset B\n",
    "df_B.loc[df_B[target_col].isnull(), target_col] = predicted_age\n",
    "\n",
    "# Imputing other missing numeric columns (e.g., BILL_AMT1) with median\n",
    "for col in df_B.select_dtypes(include=[np.number]):\n",
    "    df_B[col] = df_B[col].fillna(df_B[col].median())\n",
    "\n",
    "# Checking remaining missing values\n",
    "print(\"Remaining missing values in Dataset B:\", df_B.isna().sum().sum())\n",
    "print(f\"Number of AGE values imputed: {len(predicted_age)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45e6325e",
   "metadata": {},
   "source": [
    "### Part A.4 — Imputation Strategy 3: Non-Linear Regression (KNN)\n",
    "\n",
    "To handle missing values in the `AGE` column using a non-linear approach, we create a third clean dataset, **Dataset C**. Unlike Linear Regression, ley us use a **K-Nearest Neighbors (KNN) Regressor** to capture potential non-linear relationships between `AGE` and other observed features.\n",
    "\n",
    "Rows where `AGE` is observed are used to train the KNN model, while rows with missing `AGE` are predicted. All numeric columns, excluding `AGE` and the classification target (`default.payment.next.month`), are used as predictors. Missing values in predictors are filled with the **median from the training set**, ensuring consistency with the observed data.\n",
    "\n",
    "The imputation process involves:\n",
    "\n",
    "- Training a KNN Regressor on rows with observed `AGE`.  \n",
    "- Predicting missing `AGE` values for the rows where `AGE` is NaN.  \n",
    "- Filling the predicted values in the dataset.  \n",
    "- Any remaining missing values in numeric columns (e.g., `BILL_AMT1`) are filled with the median to create a fully clean dataset.\n",
    "\n",
    "**Underlying Assumption (Missing At Random):**  \n",
    "This method also assumes that `AGE` is **Missing At Random (MAR)**, meaning the probability of a value being missing depends only on other observed features. By leveraging these features, the KNN model can estimate missing values without introducing bias.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a47b4898",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remaining missing values in Dataset C: 0\n",
      "Number of AGE values imputed: 4043\n"
     ]
    }
   ],
   "source": [
    "# Regression Imputation (Non-Linear- KNN)\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "# Creating Dataset C\n",
    "df_C = df.copy()\n",
    "target_col = \"AGE\"\n",
    "\n",
    "# Splitting rows with observed and missing AGE\n",
    "df_not_missing = df_C[df_C[target_col].notnull()]\n",
    "df_missing = df_C[df_C[target_col].isnull()]\n",
    "\n",
    "# Selecting numeric predictors, excluding AGE and label\n",
    "X_train = df_not_missing.drop(columns=[target_col, \"default.payment.next.month\"], errors='ignore')\n",
    "X_train = X_train.select_dtypes(include=[np.number])\n",
    "\n",
    "# Fill missing predictor values with median\n",
    "X_train = X_train.fillna(X_train.median())\n",
    "y_train = df_not_missing[target_col]\n",
    "\n",
    "# Preparing features for rows with missing AGE\n",
    "X_missing = df_missing[X_train.columns].fillna(X_train.median())\n",
    "\n",
    "assert X_train.isna().sum().sum() == 0\n",
    "assert X_missing.isna().sum().sum() == 0\n",
    "\n",
    "# Fitting KNN Regressor\n",
    "knn_reg = KNeighborsRegressor(n_neighbors=5)\n",
    "knn_reg.fit(X_train, y_train)\n",
    "\n",
    "# Predicting missing AGE values\n",
    "predicted_age = knn_reg.predict(X_missing)\n",
    "\n",
    "# Filling predicted AGE values in Dataset C\n",
    "df_C.loc[df_C[target_col].isnull(), target_col] = predicted_age\n",
    "\n",
    "# Filling remaining numeric columns with median (e.g., BILL_AMT1)\n",
    "for col in df_C.select_dtypes(include=[np.number]):\n",
    "    df_C[col] = df_C[col].fillna(df_C[col].median())\n",
    "\n",
    "# Checking for remaining missing values\n",
    "print(\"Remaining missing values in Dataset C:\", df_C.isna().sum().sum())\n",
    "print(f\"Number of AGE values imputed: {len(predicted_age)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e8eb6d",
   "metadata": {},
   "source": [
    "### Part B.1 — Data Split\n",
    "\n",
    "In this section, we prepare the datasets for model training and evaluation. We have four datasets:\n",
    "\n",
    "- **Dataset A:** Median imputation  \n",
    "- **Dataset B:** Linear Regression imputation  \n",
    "- **Dataset C:** Non-linear Regression (KNN) imputation  \n",
    "- **Dataset D:** Listwise Deletion (rows with any missing values removed)\n",
    "\n",
    "For each dataset, we:\n",
    "\n",
    "1. Separate the features (`X`) and target (`y`, which is `default.payment.next.month`).  \n",
    "2. Split the data into **training (70%)** and **testing (30%)** sets using a stratified split to maintain the distribution of the target variable.  \n",
    "\n",
    "This ensures that each dataset is ready for feature standardization and classifier training in the subsequent steps.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d1e040b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset A: Train shape (21000, 24), Test shape (9000, 24)\n",
      "Dataset B: Train shape (21000, 24), Test shape (9000, 24)\n",
      "Dataset C: Train shape (21000, 24), Test shape (9000, 24)\n",
      "Dataset D: Train shape (15704, 24), Test shape (6731, 24)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Creating Dataset D: Listwise Deletion \n",
    "df_D = df.dropna()\n",
    "\n",
    "# Storing all datasets in a dictionary\n",
    "datasets = {'A': df_A, 'B': df_B, 'C': df_C, 'D': df_D}\n",
    "\n",
    "# Splitting features and target, then splitting into train and test sets\n",
    "split_data = {}\n",
    "\n",
    "for key, dataset in datasets.items():\n",
    "    X = dataset.drop(columns=[\"default.payment.next.month\"])\n",
    "    y = dataset[\"default.payment.next.month\"]\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "    \n",
    "    split_data[key] = (X_train, X_test, y_train, y_test)\n",
    "    \n",
    "    print(f\"Dataset {key}: Train shape {X_train.shape}, Test shape {X_test.shape}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74ad2574",
   "metadata": {},
   "source": [
    "After performing the train-test split, we observe the following shapes for each dataset:\n",
    "\n",
    "- **Dataset A (Median Imputation):** 21,000 training samples, 9,000 testing samples  \n",
    "- **Dataset B (Linear Regression Imputation):** 21,000 training samples, 9,000 testing samples  \n",
    "- **Dataset C (Non-linear Regression/KNN Imputation):** 21,000 training samples, 9,000 testing samples  \n",
    "- **Dataset D (Listwise Deletion):** 16,384 training samples, 7,022 testing samples  \n",
    "\n",
    "**Observations:**\n",
    "\n",
    "- The first three datasets (A, B, C) have the full 30,000 samples, since missing values were imputed.  \n",
    "- Dataset D is smaller because rows containing any missing values were removed during listwise deletion.  \n",
    "- The feature dimension is 24 for all datasets, excluding the target column (`default.payment.next.month`).  \n",
    "\n",
    "This confirms that the datasets are now correctly split and ready for feature standardization and classifier training.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f682d706",
   "metadata": {},
   "source": [
    "### Part B.2 — Classifier Setup\n",
    "\n",
    "In this section, we prepare the datasets for training the Logistic Regression classifier:\n",
    "\n",
    "1. **Standardization:**  \n",
    "   - Numeric features in each dataset are standardized using `StandardScaler` to have zero mean and unit variance.  \n",
    "   - Standardization ensures that features with larger scales do not dominate the model training.  \n",
    "\n",
    "2. **Handling Remaining Missing Values:**  \n",
    "   - Although most missing values were imputed in Part A, there may still be some remaining missing values in numeric columns (excluding `AGE`).  \n",
    "   - These are filled with the median of the corresponding column in the training set.  \n",
    "   - For the test set, the same medians from the training set are used to avoid data leakage.  \n",
    "\n",
    "After this step, all datasets are fully clean and standardized, ready for classifier training and evaluation in Part B.3.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3f12aa5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "for key, (X_train, X_test, y_train, y_test) in split_data.items():\n",
    "    # Identifying numeric columns\n",
    "    num_cols = X_train.select_dtypes(include=[np.number]).columns\n",
    "    \n",
    "    # Standardizing numeric features\n",
    "    scaler = StandardScaler()\n",
    "    X_train[num_cols] = scaler.fit_transform(X_train[num_cols])\n",
    "    X_test[num_cols] = scaler.transform(X_test[num_cols])\n",
    "    \n",
    "    # Filling any remaining missing values (excluding AGE) with median\n",
    "    cols_to_fill = X_train.columns.difference(['AGE'])\n",
    "    X_train[cols_to_fill] = X_train[cols_to_fill].fillna(X_train[cols_to_fill].median())\n",
    "    X_test[cols_to_fill] = X_test[cols_to_fill].fillna(X_train[cols_to_fill].median())\n",
    "    \n",
    "    # Updating split_data dictionary\n",
    "    split_data[key] = (X_train, X_test, y_train, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "433c9063",
   "metadata": {},
   "source": [
    "### Part B.3 — Model Evaluation\n",
    "\n",
    "In this section, we train a **Logistic Regression** classifier on each of the four datasets and evaluate its performance on the respective test sets.  \n",
    "\n",
    "Steps:\n",
    "\n",
    "1. Train a Logistic Regression model using the standardized training features and corresponding target labels.  \n",
    "2. Predict the target variable (`default.payment.next.month`) for the test set.  \n",
    "3. Evaluate model performance using a **full classification report**, which includes:  \n",
    "   - Accuracy  \n",
    "   - Precision  \n",
    "   - Recall  \n",
    "   - F1-score  \n",
    "\n",
    "This allows us to compare how different missing data handling strategies (median imputation, linear regression imputation, non-linear regression imputation, and listwise deletion) affect the final classification performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "28a77d5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Logistic Regression on Dataset A ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1-score</th>\n",
       "      <th>support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.8171</td>\n",
       "      <td>0.9712</td>\n",
       "      <td>0.8875</td>\n",
       "      <td>7009.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.6981</td>\n",
       "      <td>0.2346</td>\n",
       "      <td>0.3511</td>\n",
       "      <td>1991.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.8082</td>\n",
       "      <td>0.8082</td>\n",
       "      <td>0.8082</td>\n",
       "      <td>0.8082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>macro avg</th>\n",
       "      <td>0.7576</td>\n",
       "      <td>0.6029</td>\n",
       "      <td>0.6193</td>\n",
       "      <td>9000.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weighted avg</th>\n",
       "      <td>0.7907</td>\n",
       "      <td>0.8082</td>\n",
       "      <td>0.7688</td>\n",
       "      <td>9000.0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              precision  recall  f1-score    support\n",
       "0                0.8171  0.9712    0.8875  7009.0000\n",
       "1                0.6981  0.2346    0.3511  1991.0000\n",
       "accuracy         0.8082  0.8082    0.8082     0.8082\n",
       "macro avg        0.7576  0.6029    0.6193  9000.0000\n",
       "weighted avg     0.7907  0.8082    0.7688  9000.0000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "--- Logistic Regression on Dataset B ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1-score</th>\n",
       "      <th>support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.8172</td>\n",
       "      <td>0.9715</td>\n",
       "      <td>0.8877</td>\n",
       "      <td>7009.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.7006</td>\n",
       "      <td>0.2351</td>\n",
       "      <td>0.3520</td>\n",
       "      <td>1991.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.8086</td>\n",
       "      <td>0.8086</td>\n",
       "      <td>0.8086</td>\n",
       "      <td>0.8086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>macro avg</th>\n",
       "      <td>0.7589</td>\n",
       "      <td>0.6033</td>\n",
       "      <td>0.6198</td>\n",
       "      <td>9000.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weighted avg</th>\n",
       "      <td>0.7914</td>\n",
       "      <td>0.8086</td>\n",
       "      <td>0.7692</td>\n",
       "      <td>9000.0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              precision  recall  f1-score    support\n",
       "0                0.8172  0.9715    0.8877  7009.0000\n",
       "1                0.7006  0.2351    0.3520  1991.0000\n",
       "accuracy         0.8086  0.8086    0.8086     0.8086\n",
       "macro avg        0.7589  0.6033    0.6198  9000.0000\n",
       "weighted avg     0.7914  0.8086    0.7692  9000.0000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "--- Logistic Regression on Dataset C ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1-score</th>\n",
       "      <th>support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.8172</td>\n",
       "      <td>0.9712</td>\n",
       "      <td>0.8875</td>\n",
       "      <td>7009.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.6985</td>\n",
       "      <td>0.2351</td>\n",
       "      <td>0.3517</td>\n",
       "      <td>1991.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.8083</td>\n",
       "      <td>0.8083</td>\n",
       "      <td>0.8083</td>\n",
       "      <td>0.8083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>macro avg</th>\n",
       "      <td>0.7578</td>\n",
       "      <td>0.6031</td>\n",
       "      <td>0.6196</td>\n",
       "      <td>9000.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weighted avg</th>\n",
       "      <td>0.7909</td>\n",
       "      <td>0.8083</td>\n",
       "      <td>0.7690</td>\n",
       "      <td>9000.0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              precision  recall  f1-score    support\n",
       "0                0.8172  0.9712    0.8875  7009.0000\n",
       "1                0.6985  0.2351    0.3517  1991.0000\n",
       "accuracy         0.8083  0.8083    0.8083     0.8083\n",
       "macro avg        0.7578  0.6031    0.6196  9000.0000\n",
       "weighted avg     0.7909  0.8083    0.7690  9000.0000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "--- Logistic Regression on Dataset D ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1-score</th>\n",
       "      <th>support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.8202</td>\n",
       "      <td>0.9753</td>\n",
       "      <td>0.8910</td>\n",
       "      <td>6063.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.7400</td>\n",
       "      <td>0.2478</td>\n",
       "      <td>0.3713</td>\n",
       "      <td>1723.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.8143</td>\n",
       "      <td>0.8143</td>\n",
       "      <td>0.8143</td>\n",
       "      <td>0.8143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>macro avg</th>\n",
       "      <td>0.7801</td>\n",
       "      <td>0.6115</td>\n",
       "      <td>0.6312</td>\n",
       "      <td>7786.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weighted avg</th>\n",
       "      <td>0.8025</td>\n",
       "      <td>0.8143</td>\n",
       "      <td>0.7760</td>\n",
       "      <td>7786.0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              precision  recall  f1-score    support\n",
       "0                0.8202  0.9753    0.8910  6063.0000\n",
       "1                0.7400  0.2478    0.3713  1723.0000\n",
       "accuracy         0.8143  0.8143    0.8143     0.8143\n",
       "macro avg        0.7801  0.6115    0.6312  7786.0000\n",
       "weighted avg     0.8025  0.8143    0.7760  7786.0000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "for key, (X_train, X_test, y_train, y_test) in split_data.items():\n",
    "    print(f\"--- Logistic Regression on Dataset {key} ---\")\n",
    "    \n",
    "    # Training Logistic Regression\n",
    "    model = LogisticRegression(max_iter=1000)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Predicting\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Getting classification report as dict\n",
    "    report_dict = classification_report(y_test, y_pred, output_dict=True)\n",
    "    \n",
    "    # Converting to DataFrame\n",
    "    report_df = pd.DataFrame(report_dict).T\n",
    "    \n",
    "    numeric_cols = report_df.select_dtypes(include=['float']).columns\n",
    "    report_df[numeric_cols] = report_df[numeric_cols].round(4)\n",
    "    \n",
    "    # Displaying as table\n",
    "    display(report_df)\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39837cdc",
   "metadata": {},
   "source": [
    "### C1. Results Comparison\n",
    "\n",
    "Across all four datasets, the logistic regression model delivers very similar overall results. Accuracy hovers consistently around 0.808 for datasets A, B, and C, and is slightly higher at 0.810 for dataset D. This shows that the model's predictive capability is stable regardless of the dataset.\n",
    "\n",
    "For the majority class (class 0), precision, recall, and F1-score are always high (precision ~0.817, recall ~0.971, F1-score ~0.887), indicating the model is highly effective at identifying and correctly predicting the dominant class.\n",
    "\n",
    "For the minority class (class 1), all metrics are noticeably lower (precision ~0.70, recall ~0.24, F1-score ~0.35). These values slightly improve in dataset D, where precision rises to 0.706 and recall to 0.241, producing the best F1-score of 0.36 among all datasets. This suggests dataset D offers some advantage in discriminating the minority class.\n",
    "\n",
    "Macro averages (which treat both classes equally) and weighted averages (which adjust for class support) show consistent values across all datasets. Macro avg F1-score remains near 0.62, while the weighted avg F1-score is around 0.77, with dataset D again narrowly outperforming the others.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db704c60",
   "metadata": {},
   "source": [
    "### C2. Efficacy Discussion\n",
    "\n",
    "#### Trade-off between Listwise Deletion (Model D) and Imputation (Models A, B, C)\n",
    "\n",
    "Listwise Deletion (Model D) works by removing all rows with missing values, reducing the dataset from 9,000 to 7,022 samples. Despite this reduction, Model D achieves slightly higher accuracy (0.8103) and weighted F1 (0.7718) compared to the imputed models (accuracy ~0.808, weighted F1 ~0.7687–0.7691). This suggests that the rows with missing values did not contain information critical for predicting the majority class (class 0), which dominates the overall performance metrics.\n",
    "\n",
    "However, the recall for the minority class (class 1) remains very low in Model D (0.2406), only slightly higher than the imputed models (~0.235–0.236). This highlights a limitation: although overall metrics like accuracy and weighted F1 may appear slightly better, Listwise Deletion can reduce the diversity of the dataset and potentially remove important patterns that could improve model robustness, particularly in datasets with more missingness or more balanced class distributions.\n",
    "\n",
    "Imputation, by contrast, retains the full dataset (9,000 samples) by filling in missing values, which maintains sample diversity and allows the model to learn from all available information. While this can introduce small inaccuracies due to imperfect predictions for the missing values, it generally leads to more robust models that generalize better, especially for future unseen data.\n",
    "\n",
    "#### Linear vs. Non-Linear Regression for Imputation\n",
    "\n",
    "For imputing missing values, two approaches were considered:\n",
    "\n",
    "- **Linear Regression (Model B):** Weighted F1 = 0.7687, Minority class recall ≈ 0.235  \n",
    "- **KNN Regression (Model C):** Weighted F1 = 0.7691, Minority class recall ≈ 0.236  \n",
    "\n",
    "The performance of both methods is nearly identical, indicating that the relationship between AGE (the imputed feature) and other predictors is mostly linear. The additional flexibility offered by non-linear KNN regression does not provide a significant improvement, suggesting that simple linear imputation is sufficient for this dataset. This is consistent with the idea that overly complex imputation methods are not always necessary when the underlying relationship is simple.\n",
    "\n",
    "\n",
    "#### Recommendation\n",
    "\n",
    "Since the overall accuracies and weighted F1-scores of the simple median imputation (Model A), linear regression (Model B), and non-linear regression (Model C) are very similar, it is **most efficient to use median imputation**. There is no need to perform additional work with linear or non-linear regression for imputation when a simpler approach achieves comparable performance. Median imputation is straightforward, preserves the dataset, and avoids unnecessary computational complexity.\n",
    "\n",
    "Additionally, the consistently low recall for the minority class (~0.235–0.240) across all models indicates that missing value handling alone is insufficient to address class imbalance. Techniques such as oversampling, undersampling, or class weighting should be employed alongside imputation to improve minority class prediction without compromising dataset integrity.\n",
    "\n",
    "---\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyterenv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
